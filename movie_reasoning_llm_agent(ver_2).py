# -*- coding: utf-8 -*-
"""update(06.11).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z-m3R7Z1_0-DB1vUOIlOH1cEw-vB0an1
"""

# -*- coding: utf-8 -*-
"""Movie Reasoning LLM Agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W4OVSTy1rXVAM1wfg1Ypp-Cf3yNbacp1
"""

# 필수 패키지 설치 셀
!pip install -U sentence-transformers
!pip install -U transformers
!pip install -U bitsandbytes
!pip install faiss-cpu
!pip install faiss-cpu
!pip install -U sentence-transformers
!pip install -U transformers
!pip install -U bitsandbytes
!pip install -U bitsandbytes

# LLM Agent 및 데이터 로딩
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
import faiss
import pickle
import numpy as np

# Google Drive 마운트
from google.colab import drive
drive.mount('/content/drive')

# 인덱스 및 메타데이터 경로 수정 (Colab Notebooks 경로 반영)
index = faiss.read_index("/content/drive/MyDrive/Colab Notebooks/movie_index.faiss")
with open("/content/drive/MyDrive/Colab Notebooks/movie_sentences.pkl", "rb") as f:
    sentences = pickle.load(f)
with open("/content/drive/MyDrive/Colab Notebooks/movie_metas.pkl", "rb") as f:
    metas = pickle.load(f)

# 모델 로드
embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
sentence_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

model_id = "NousResearch/Meta-Llama-3-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype="auto")
llama3 = pipeline("text-generation", model=model, tokenizer=tokenizer)

# 키워드 추출 함수 및 결과 출력 함수 정의

# LLaMA3 키워드 추출
def extract_keywords_llama3(text, model_pipe):
    prompt = f"[INST] 다음 문장은 사용자가 기억하는 영화 설명입니다.:\n\n\"{text}\"\n이 문장에서 중요한 단어 또는 구절을 3~5개 **한국어로** 추출하세요.
장르, 감정, 인물 관계, 주요 사건 등을 중심으로.
출력은 쉼표로 구분된 **한 줄의 한국어 단어**로만 해주세요.
예: 액션, 공항, 테러, 납치, 승객 [/INST]"
    try:
        output = model_pipe(prompt, max_new_tokens=50)[0]['generated_text']
        extracted = output.split("Answer:")[-1].strip().replace("[/INST]", "").strip()
        keywords = [kw.strip() for kw in extracted.split(",") if kw.strip()]
        return keywords
    except Exception as e:
        print("키워드 추출 실패:", e)
        return []

## 추가한 부분
def generate_summary_with_llama3(movie_info_list, model_pipe):
    summary_input = "다음은 검색된 영화 리스트입니다:\n"
    for idx, info in enumerate(movie_info_list):
        summary_input += f"{idx+1}. 제목: {info.get('title', '')}, 장르: {info.get('genre', '')}, 연도: {info.get('year', '')}\n"
    summary_input += "\n이 영화들의 공통점이나 특징을 요약해 주세요. 간단히 3~4줄."

    try:
        result = model_pipe(summary_input, max_new_tokens=150)[0]['generated_text']
        return result.strip()
    except Exception as e:
        print("요약 실패:", e)
        return ""

# 검색 결과 출력 함수
def safe_print_result(idx, score=None):
    print(f"문장: {sentences[idx]}")
    if score is not None:
        print(f"유사도 거리: {score:.4f}")
    try:
        print(f"제목: {metas[idx]['title']}")
    except:
        print("제목: [정보 없음]")
    try:
        print(f"장르: {metas[idx]['genre']}")
    except:
        print("장르: [정보 없음]")
    try:
        print(f"연도: {metas[idx]['year']}")
    except:
        print("연도: [정보 없음]")
    print("-" * 40)

# 검색 실행 함수 정의
# 문장 전체 기반 검색 실행 함수
def run_search_sentence_only():
    user_input = input("기억나는 영화 설명을 자유롭게 입력해주세요: ")

    # 문장 전체 임베딩 생성
    sent_vec = sentence_model.encode([user_input]).astype('float32')

    # FAISS 검색
    D_sent, I_sent = index.search(sent_vec, k=10)

    # 결과 출력 (중복 제거)
    seen = set()
    print("\n[문장 전체 기반 검색 결과]")
    for d, i in sorted(zip(D_sent[0], I_sent[0]), key=lambda x: x[0]):
        title = metas[i].get("title", "")
        if title not in seen:
            safe_print_result(i, score=d)
            seen.add(title)

## 추가한 부분
def run_search_with_llama():
    user_input = input("기억나는 영화 설명을 입력하세요: ")

    keywords = extract_keywords_llama3(user_input, llama3)
    print("추출된 키워드:", keywords)

    if not keywords:
        print("키워드 추출 실패. 종료합니다.")
        return

    keyword_query = " ".join(keywords)
    keyword_vec = sentence_model.encode([keyword_query]).astype('float32')
    D, I = index.search(keyword_vec, k=5)

    results = []
    print("\n[키워드 기반 검색 결과]")
    for score, i in sorted(zip(D[0], I[0]), key=lambda x: x[0]):
        results.append(metas[i])
        print("제목:", metas[i].get("title", "[제목 없음]"))
        print("장르:", metas[i].get("genre", "[장르 없음]"))
        print("연도:", metas[i].get("year", "[연도 없음]"))
        print(f"유사도 거리: {score:.4f}")
        print("-----")

    # print("\nLLaMA3 요약 결과:")
    # summary = generate_summary_with_llama3(results, llama3)
    # print(summary)

run_search_sentence_only()

# 실행
run_search_with_llama()