# -*- coding: utf-8 -*-
"""Movie Reasoning LLM Agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W4OVSTy1rXVAM1wfg1Ypp-Cf3yNbacp1
"""

# 필수 패키지 설치 셀
!pip install -U sentence-transformers
!pip install -U transformers
!pip install -U bitsandbytes
!pip install faiss-cpu
!pip install faiss-cpu
!pip install -U sentence-transformers
!pip install -U transformers
!pip install -U bitsandbytes
!pip install -U bitsandbytes

# LLM Agent 및 데이터 로딩
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
import faiss
import pickle
import numpy as np

# Google Drive 마운트
from google.colab import drive
drive.mount('/content/drive')

# 인덱스 및 메타데이터 경로 수정 (Colab Notebooks 경로 반영)
index = faiss.read_index("/content/drive/MyDrive/Colab Notebooks/movie_index.faiss")
with open("/content/drive/MyDrive/Colab Notebooks/movie_sentences.pkl", "rb") as f:
    sentences = pickle.load(f)
with open("/content/drive/MyDrive/Colab Notebooks/movie_metas.pkl", "rb") as f:
    metas = pickle.load(f)

# 모델 로드
embedding_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
sentence_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

model_id = "NousResearch/Meta-Llama-3-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype="auto")
llama3 = pipeline("text-generation", model=model, tokenizer=tokenizer)

# 키워드 추출 함수 및 결과 출력 함수 정의

# LLaMA3 키워드 추출
def extract_keywords_llama3(text, model_pipe):
    prompt = f"[INST] 다음 문장은 사용자가 기억하는 영화 설명입니다.:\n\n\"{text}\"\n이 문장에서 중요한 키워드를 3~5개 추출해주세요. 장르, 감정, 인물 관계, 주요 사건 등을 중심으로. 쉼표로 구분해서 한 줄로 출력해주세요. [/INST]"
    try:
        output = model_pipe(prompt, max_new_tokens=50)[0]['generated_text']
        extracted = output.split("Answer:")[-1].strip().replace("[/INST]", "").strip()
        keywords = [kw.strip() for kw in extracted.split(",") if kw.strip()]
        return keywords
    except Exception as e:
        print("키워드 추출 실패:", e)
        return []

# 검색 결과 출력 함수
def safe_print_result(idx, score=None):
    print(f"문장: {sentences[idx]}")
    if score is not None:
        print(f"유사도 거리: {score:.4f}")
    try:
        print(f"제목: {metas[idx]['title']}")
    except:
        print("제목: [정보 없음]")
    try:
        print(f"장르: {metas[idx]['genre']}")
    except:
        print("장르: [정보 없음]")
    try:
        print(f"연도: {metas[idx]['year']}")
    except:
        print("연도: [정보 없음]")
    print("-" * 40)

# 검색 실행 함수 정의
# 문장 전체 기반 검색 실행 함수
def run_search_sentence_only():
    user_input = input("기억나는 영화 설명을 자유롭게 입력해주세요: ")

    # 문장 전체 임베딩 생성
    sent_vec = sentence_model.encode([user_input]).astype('float32')

    # FAISS 검색
    D_sent, I_sent = index.search(sent_vec, k=10)

    # 결과 출력 (중복 제거)
    seen = set()
    print("\n[문장 전체 기반 검색 결과]")
    for d, i in sorted(zip(D_sent[0], I_sent[0]), key=lambda x: x[0]):
        title = metas[i].get("title", "")
        if title not in seen:
            safe_print_result(i, score=d)
            seen.add(title)

# 실행
run_search_sentence_only()